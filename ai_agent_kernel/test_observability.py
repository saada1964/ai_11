#!/usr/bin/env python3
"""
Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (Advanced Observability System)
ÙŠØ¯Ø±Ø³ Ø¬Ù…ÙŠØ¹ Ù…ÙŠØ²Ø§Øª OpenTelemetryØŒ distributed tracingØŒ ÙˆØ§Ù„Ù€ metrics collection
"""

import asyncio
import json
import time
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any
from unittest.mock import AsyncMock, MagicMock, patch, Mock

# Mock dependencies to run tests
os.environ['TESTING'] = 'true'

# Import core modules
from core.observability import observability_manager, MetricCollector, TraceLevel, MetricType
from core.orchestrator import Orchestrator
from core.planner import Planner
from core.executor import Executor


class TestObservabilitySystem:
    """Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
    
    def setup_method(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø¨Ù„ ÙƒÙ„ Ø§Ø®ØªØ¨Ø§Ø±"""
        # Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
        observability_manager.metric_collector.metrics.clear()
        observability_manager.metric_collector.counters.clear()
        observability_manager.metric_collector.gauges.clear()
        observability_manager.metric_collector.histograms.clear()
        observability_manager.traces.clear()
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø­Ø§Ù„Ø©
        observability_manager.performance_monitor.reset()
        observability_manager.trace_processor.reset()
    
    def test_metric_collection(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
        print("ğŸ“Š Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³...")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Counter metric
        observability_manager.record_metric(
            "test.counter", 5.0, 
            observability_manager.metric_collector.metric_type.COUNTER,
            {"label": "test_value"}
        )
        
        # Ø§Ø®ØªØ¨Ø§Ø± Gauge metric
        observability_manager.record_metric(
            "test.gauge", 42.0,
            observability_manager.metric_collector.metric_type.GAUGE,
            {"component": "test_component"}
        )
        
        # Ø§Ø®ØªØ¨Ø§Ø± Histogram metric
        observability_manager.record_histogram(
            "test.histogram", 10.5,
            {"operation": "test_op"}
        )
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        assert len(observability_manager.metric_collector.metrics) >= 3
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Counter
        counter_key = observability_manager.metric_collector._make_key("test.counter", {"label": "test_value"})
        assert observability_manager.metric_collector.counters[counter_key] == 5.0
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Gauge
        gauge_key = observability_manager.metric_collector._make_key("test.gauge", {"component": "test_component"})
        assert observability_manager.metric_collector.gauges[gauge_key] == 42.0
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¨Ù†Ø¬Ø§Ø­")
    
    async def test_trace_operations(self):
        """Ø§Ø®ØªØ¨Ø§Ø± ØªØªØ¨Ø¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª (Trace Operations)"""
        print("ğŸ” Ø§Ø®ØªØ¨Ø§Ø± ØªØªØ¨Ø¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª...")
        
        # Ø¥Ù†Ø´Ø§Ø¡ trace operation
        async with observability_manager.trace_operation(
            "test_operation",
            {"test_param": "value"},
            TraceLevel.HIGH
        ):
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø¹Ù…Ù„ÙŠØ©
            await asyncio.sleep(0.01)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­ÙØ¸ Trace
        assert len(observability_manager.traces) > 0
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù€ trace Ø§Ù„Ù…Ù†Ø´Ø£
        test_trace = None
        for trace in observability_manager.traces:
            if trace.operation_name == "test_operation":
                test_trace = trace
                break
        
        assert test_trace is not None
        assert test_trace.status == "success"
        assert test_trace.level == TraceLevel.HIGH
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± ØªØªØ¨Ø¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ù†Ø¬Ø§Ø­")
    
    async def test_trace_levels(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
        print("ğŸ“ˆ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØªØªØ¨Ø¹...")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
        levels_to_test = [TraceLevel.LOW, TraceLevel.MEDIUM, TraceLevel.HIGH, TraceLevel.CRITICAL]
        
        for level in levels_to_test:
            async with observability_manager.trace_operation(
                f"test_level_{level.value}",
                level=level
            ):
                await asyncio.sleep(0.001)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ traces
        assert len(observability_manager.traces) >= 4
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø³ØªÙˆÙŠØ§Øª Ù…Ø®ØªÙ„ÙØ©
        traces_by_level = {}
        for trace in observability_manager.traces:
            if trace.level not in traces_by_level:
                traces_by_level[trace.level] = []
            traces_by_level[trace.level].append(trace)
        
        for level in levels_to_test:
            assert level in traces_by_level
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØªØªØ¨Ø¹ Ø¨Ù†Ø¬Ø§Ø­")
    
    async def test_performance_monitoring(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        print("âš¡ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡...")
        
        # ØªØ³Ø¬ÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø¨Ø·ÙŠØ¦Ø©
        start_time = time.time()
        async with observability_manager.trace_operation("slow_operation", level=TraceLevel.HIGH):
            await asyncio.sleep(0.05)  # Ù…Ø­Ø§ÙƒØ§Ø© Ø¹Ù…Ù„ÙŠØ© Ø¨Ø·ÙŠØ¦Ø©
        
        end_time = time.time()
        
        # ØªØ³Ø¬ÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø³Ø±ÙŠØ¹Ø©
        async with observability_manager.trace_operation("fast_operation", level=TraceLevel.MEDIUM):
            await asyncio.sleep(0.01)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        performance_metrics = observability_manager.get_performance_metrics()
        assert "slow_operation" in performance_metrics["traces"]
        assert "fast_operation" in performance_metrics["traces"]
        
        slow_trace_duration = performance_metrics["traces"]["slow_operation"]["duration"]
        fast_trace_duration = performance_metrics["traces"]["fast_operation"]["duration"]
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ø·ÙŠØ¦Ø© Ø£Ø¨Ø·Ø£ Ù…Ù† Ø§Ù„Ø³Ø±ÙŠØ¹Ø©
        assert slow_trace_duration > fast_trace_duration
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨Ù†Ø¬Ø§Ø­")
    
    def test_system_status(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…"""
        print("ğŸ“‹ Ø§Ø®ØªØ¨Ø§Ø± Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…...")
        
        # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¹Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        observability_manager.record_metric("test.metric", 1.0, MetricType.COUNTER)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
        status = observability_manager.get_system_status()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø­ØªÙˆÙŠØ§Øª Ø§Ù„Ø­Ø§Ù„Ø©
        assert "system_health" in status
        assert "metrics" in status
        assert "traces" in status
        assert "performance" in status
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        metrics_stats = status["metrics"]
        assert metrics_stats["total_metrics"] > 0
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø¬Ø§Ø­")
    
    async def test_error_handling_in_traces(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„ØªØªØ¨Ø¹"""
        print("âŒ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡...")
        
        # Ø¥Ù†Ø´Ø§Ø¡ trace Ø¨Ø®Ø·Ø£
        try:
            async with observability_manager.trace_operation("failing_operation", level=TraceLevel.HIGH):
                raise ValueError("Test error")
        except ValueError:
            pass
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­ÙØ¸ Ø§Ù„Ù€ trace Ù…Ø¹ Ø­Ø§Ù„Ø© error
        test_trace = None
        for trace in observability_manager.traces:
            if trace.operation_name == "failing_operation":
                test_trace = trace
                break
        
        assert test_trace is not None
        assert test_trace.status == "error"
        assert "Test error" in str(test_tracemeta_data.get("error", ""))
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø¨Ù†Ø¬Ø§Ø­")
    
    async def test_orchestrator_integration(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†Ø³Ù‚"""
        print("ğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†Ø³Ù‚...")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù†Ø³Ù‚
        orchestrator = Orchestrator()
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        initial_traces_count = len(observability_manager.traces)
        
        async with observability_manager.trace_operation("orchestrator_integration_test", level=TraceLevel.HIGH):
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ù†Ø³Ù‚
            async with observability_manager.trace_operation("get_user_memory", level=TraceLevel.MEDIUM):
                await asyncio.sleep(0.001)
            
            async with observability_manager.trace_operation("get_conversation_context", level=TraceLevel.MEDIUM):
                await asyncio.sleep(0.001)
            
            async with observability_manager.trace_operation("load_configurations", level=TraceLevel.MEDIUM):
                await asyncio.sleep(0.001)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ traces Ø¥Ø¶Ø§ÙÙŠØ©
        final_traces_count = len(observability_manager.traces)
        assert final_traces_count > initial_traces_count
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† traces Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
        orchestrator_trace_found = False
        sub_operation_traces = 0
        
        for trace in observability_manager.traces:
            if trace.operation_name == "orchestrator_integration_test":
                orchestrator_trace_found = True
            elif trace.operation_name in ["get_user_memory", "get_conversation_context", "load_configurations"]:
                sub_operation_traces += 1
        
        assert orchestrator_trace_found
        assert sub_operation_traces >= 3
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†Ø³Ù‚ Ø¨Ù†Ø¬Ø§Ø­")
    
    async def test_planner_integration(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø®Ø·Ø·"""
        print("ğŸ“ Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø®Ø·Ø·...")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø·
        planner = Planner()
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        initial_traces_count = len(observability_manager.traces)
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø·Ø©
        async with observability_manager.trace_operation("planner.create_plan", level=TraceLevel.HIGH):
            async with observability_manager.trace_operation("generate_initial_plan", level=TraceLevel.MEDIUM):
                await asyncio.sleep(0.001)
            
            async with observability_manager.trace_operation("apply_dynamic_memory", level=TraceLevel.MEDIUM):
                await asyncio.sleep(0.001)
            
            if planner.enable_self_correction:
                async with observability_manager.trace_operation("apply_self_correction", level=TraceLevel.MEDIUM):
                    await asyncio.sleep(0.001)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ traces
        final_traces_count = len(observability_manager.traces)
        assert final_traces_count > initial_traces_count
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ traces Ø§Ù„Ù…Ø®Ø·Ø·
        planner_traces = [
            trace for trace in observability_manager.traces 
            if trace.operation_name.startswith("planner.") or 
               trace.operation_name in ["generate_initial_plan", "apply_dynamic_memory", "apply_self_correction"]
        ]
        
        assert len(planner_traces) >= 3
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø®Ø·Ø· Ø¨Ù†Ø¬Ø§Ø­")
    
    async def test_executor_integration(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†ÙØ°"""
        print("âš™ï¸ Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†ÙØ°...")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù†ÙØ°
        executor = Executor()
        
        # Ù…Ø­Ø§ÙƒØ§Ø© ØªÙ†ÙÙŠØ° Ø®Ø·Ø©
        test_plan = {
            "plan": {
                "steps": [
                    {"id": "step_1", "type": "TOOL_CALL", "tool": "test_tool"},
                    {"id": "step_2", "type": "DIRECT_ANSWER", "description": "Test step"}
                ]
            }
        }
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ†ÙÙŠØ° Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        initial_traces_count = len(observability_manager.traces)
        
        async with observability_manager.trace_operation("executor.execute_plan", level=TraceLevel.HIGH):
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ°
            async with observability_manager.trace_operation("validate_plan", level=TraceLevel.MEDIUM):
                await asyncio.sleep(0.001)
            
            async with observability_manager.trace_operation("build_dependency_graph", level=TraceLevel.LOW):
                await asyncio.sleep(0.001)
            
            async with observability_manager.trace_operation("topological_sort", level=TraceLevel.LOW):
                await asyncio.sleep(0.001)
            
            # Ù…Ø­Ø§ÙƒØ§Ø© ØªÙ†ÙÙŠØ° Ø§Ù„Ø®Ø·ÙˆØ§Øª
            for i, step in enumerate(test_plan["plan"]["steps"]):
                step_id = step.get("id", f"index_{i}")
                async with observability_manager.trace_operation(
                    f"execute_step_{step_id}",
                    level=TraceLevel.HIGH,
                    labels={"step_id": step_id, "step_type": step.get("type", "unknown")}
                ):
                    await asyncio.sleep(0.001)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ traces
        final_traces_count = len(observability_manager.traces)
        assert final_traces_count > initial_traces_count
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† traces Ø§Ù„Ù…Ù†ÙØ°
        executor_traces = [
            trace for trace in observability_manager.traces 
            if trace.operation_name.startswith("executor.") or 
               "execute_step_" in trace.operation_name
        ]
        
        assert len(executor_traces) >= 4  # validate_plan + build_dependency_graph + topological_sort + step executions
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†ÙØ° Ø¨Ù†Ø¬Ø§Ø­")
    
    def test_metric_export_format(self):
        """Ø§Ø®ØªØ¨Ø§Ø± ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØµØ­ÙŠØ­"""
        print("ğŸ“¤ Ø§Ø®ØªØ¨Ø§Ø± ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³...")
        
        # ØªØ³Ø¬ÙŠÙ„ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…ØªÙ†ÙˆØ¹Ø©
        observability_manager.record_metric("test.counter", 10.0, MetricType.COUNTER)
        observability_manager.record_metric("test.gauge", 25.0, MetricType.GAUGE)
        observability_manager.record_histogram("test.histogram", 15.0)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…ØµØ¯Ø±Ø©
        metrics_export = observability_manager.export_metrics()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨Ù†ÙŠØ©
        assert "metrics" in metrics_export
        assert "counters" in metrics_export
        assert "gauges" in metrics_export
        assert "histograms" in metrics_export
        assert "total_count" in metrics_export
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        assert metrics_export["total_count"] >= 3
        assert len(metrics_export["counters"]) > 0
        assert len(metrics_export["gauges"]) > 0
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¨Ù†Ø¬Ø§Ø­")
    
    async def test_complex_scenario(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ù…Ø¹Ù‚Ø¯ Ø´Ø§Ù…Ù„"""
        print("ğŸŒŸ Ø§Ø®ØªØ¨Ø§Ø± Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ù…Ø¹Ù‚Ø¯ Ø´Ø§Ù…Ù„...")
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø·Ù„Ø¨ ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø§Ø­Ù„
        start_time = time.time()
        
        async with observability_manager.trace_operation("complete_request", level=TraceLevel.CRITICAL):
            # Ù…Ø±Ø­Ù„Ø© ØªØ®Ø·ÙŠØ·
            async with observability_manager.trace_operation("planning_phase", level=TraceLevel.HIGH):
                await asyncio.sleep(0.02)
                observability_manager.record_metric("planning.time", 0.02, MetricType.GAUGE)
            
            # Ù…Ø±Ø­Ù„Ø© ØªÙ†ÙÙŠØ°
            async with observability_manager.trace_operation("execution_phase", level=TraceLevel.HIGH):
                for i in range(3):
                    async with observability_manager.trace_operation(
                        f"execution_step_{i+1}",
                        level=TraceLevel.MEDIUM,
                        labels={"step": str(i+1)}
                    ):
                        await asyncio.sleep(0.01)
                        observability_manager.record_metric(
                            f"execution.step_{i+1}", 1.0, 
                            MetricType.COUNTER, {"step": str(i+1)}
                        )
            
            # Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            async with observability_manager.trace_operation("response_phase", level=TraceLevel.MEDIUM):
                await asyncio.sleep(0.01)
                observability_manager.record_metric("response.generated", 1.0, MetricType.COUNTER)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        all_traces = observability_manager.traces
        all_metrics = observability_manager.metric_collector.metrics
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø±Ø§Ø­Ù„
        trace_names = [trace.operation_name for trace in all_traces]
        assert "complete_request" in trace_names
        assert "planning_phase" in trace_names
        assert "execution_phase" in trace_names
        assert "response_phase" in trace_names
        assert any("execution_step_" in name for name in trace_names)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        metric_names = [metric.name for metric in all_metrics]
        assert "planning.time" in metric_names
        assert "response.generated" in metric_names
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø³ØªÙˆÙ‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        main_trace = next(trace for trace in all_traces if trace.operation_name == "complete_request")
        assert main_trace.level == TraceLevel.CRITICAL
        
        print("âœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ù…Ø¹Ù‚Ø¯ Ø¨Ù†Ø¬Ø§Ø­")


def run_async_tests():
    """ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©"""
    async def run_all_tests():
        test_instance = TestObservabilitySystem()
        test_instance.setup_method()
        
        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
        async_tests = [
            ("ØªØªØ¨Ø¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª", test_instance.test_trace_operations),
            ("Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØªØªØ¨Ø¹", test_instance.test_trace_levels),
            ("Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡", test_instance.test_performance_monitoring),
            ("Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡", test_instance.test_error_handling_in_traces),
            ("Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†Ø³Ù‚", test_instance.test_orchestrator_integration),
            ("Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø®Ø·Ø·", test_instance.test_planner_integration),
            ("Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù†ÙØ°", test_instance.test_executor_integration),
            ("Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ù…Ø¹Ù‚Ø¯", test_instance.test_complex_scenario)
        ]
        
        results = []
        
        for test_name, test_func in async_tests:
            print(f"\nğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø±: {test_name}")
            try:
                await test_func()
                results.append((test_name, "âœ… Ù†Ø¬Ø­", None))
                print(f"   {test_name}: âœ… Ù†Ø¬Ø­")
            except Exception as e:
                results.append((test_name, "âŒ ÙØ´Ù„", str(e)))
                print(f"   {test_name}: âŒ ÙØ´Ù„ - {str(e)}")
        
        return results
    
    return asyncio.run(run_all_tests())


def run_sync_tests():
    """ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©"""
    test_instance = TestObservabilitySystem()
    test_instance.setup_method()
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
    sync_tests = [
        ("Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³", test_instance.test_metric_collection),
        ("Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…", test_instance.test_system_status),
        ("ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³", test_instance.test_metric_export_format)
    ]
    
    results = []
    
    for test_name, test_func in sync_tests:
        print(f"\nğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø±: {test_name}")
        try:
            test_func()
            results.append((test_name, "âœ… Ù†Ø¬Ø­", None))
            print(f"   {test_name}: âœ… Ù†Ø¬Ø­")
        except Exception as e:
            results.append((test_name, "âŒ ÙØ´Ù„", str(e)))
            print(f"   {test_name}: âŒ ÙØ´Ù„ - {str(e)}")
    
    return results


def run_all_tests():
    """ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©...")
    print("=" * 60)
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© Ø£ÙˆÙ„Ø§Ù‹
    sync_results = run_sync_tests()
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
    async_results = run_async_tests()
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    all_results = sync_results + async_results
    
    # Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("\n" + "=" * 60)
    print("ğŸ“‹ Ù…Ù„Ø®Øµ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª:")
    print("=" * 60)
    
    passed = 0
    failed = 0
    
    for test_name, result, error in all_results:
        status_emoji = "âœ…" if "Ù†Ø¬Ø­" in result else "âŒ"
        print(f"{status_emoji} {test_name}: {result}")
        if error:
            print(f"   Ø§Ù„ØªÙØ§ØµÙŠÙ„: {error}")
        
        if "Ù†Ø¬Ø­" in result:
            passed += 1
        else:
            failed += 1
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print(f"   Ù†Ø¬Ø­: {passed}")
    print(f"   ÙØ´Ù„: {failed}")
    print(f"   Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹: {passed + failed}")
    
    if failed == 0:
        print("ğŸ‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ø¬Ø­Øª!")
    else:
        print(f"âš ï¸ {failed} Ø§Ø®ØªØ¨Ø§Ø± ÙØ´Ù„")
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
    print("\nğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©:")
    print(f"   Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©: {len(observability_manager.metric_collector.metrics)}")
    print(f"   Ø§Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù…Ø­ÙÙˆØ¸: {len(observability_manager.traces)}")
    
    # Ø¹Ø±Ø¶ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
    if observability_manager.metric_collector.metrics:
        print("\nğŸ“Š Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³:")
        for metric in observability_manager.metric_collector.metrics[:5]:
            print(f"   - {metric.name}: {metric.value} ({metric.metric_type.value})")
    
    return all_results


if __name__ == "__main__":
    run_all_tests()